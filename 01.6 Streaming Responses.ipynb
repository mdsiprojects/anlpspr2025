{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from openai import embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-proj-eUanLfMUnbXn ...\n",
      "org-8d28 ...\n",
      "proj_30j ...\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv(\"OPENAI_API_KEY\")[:20], \"...\")\n",
    "print(os.getenv(\"OPENAI_ORG\")[:8], \"...\")\n",
    "print(os.getenv(\"OPENAI_PROJECT\")[:8], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Envinronment Variables\n",
    "\n",
    "Create a .env File: In the root of your project directory, create a file named `.env`. This file will hold your environment variables in the form of key-value pairs. For example:\n",
    "\n",
    "```ini\n",
    "    OPENAI_API_KEY = <openai_apikey>\n",
    "\n",
    "\n",
    "    # optional \n",
    "    OPENAI_ORG = <openai_org_id>\n",
    "    OPENAI_PROJECT = <openai_project_id>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the API key and endpoint\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "org = os.getenv('OPENAI_ORG')\n",
    "project = os.getenv('OPENAI_PROJECT')\n",
    "# model = \"gpt-5-mini-2025-08-07\"\n",
    "model = \"gpt-4.1-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Client\n",
    "Azure OpenAI provides two methods for authentication. You can use either API Keys or Microsoft Entra ID.\n",
    "\n",
    "* API Key authentication: For this type of authentication, all API requests must include the API Key in the api-key HTTP header. \n",
    "\n",
    "* Microsoft Entra ID authentication: You can authenticate an API call using a Microsoft Entra token. Authentication tokens are included in a request as the Authorization header. The token provided must be preceded by Bearer, for example Bearer YOUR_AUTH_TOKEN. You can read our how-to guide on authenticating with Microsoft Entra ID.\n",
    "\n",
    "We will be using API Key for this class. Store the API Key in the `.env` file, never share your API Key with others. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    organization=org,\n",
    "    project=project\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt-4.1-2025-04-14',\n",
       " 'gpt-4.1',\n",
       " 'gpt-4.1-mini-2025-04-14',\n",
       " 'gpt-4.1-mini',\n",
       " 'gpt-4.1-nano-2025-04-14',\n",
       " 'gpt-4.1-nano']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m.id for m in client.models.list().data if m.id.startswith(\"gpt-4.1\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Responses API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=model,\n",
    "    input=\"Hello, world!\"\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "\n",
    "                { \"type\": \"input_text\", \"text\": \"describe \" },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
    "                }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "response = client.responses.create(\n",
    "    model=model,\n",
    "    input=input\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows a long wooden boardwalk cutting through a vast, lush green field. The grass on either side is tall and vibrant, with some scattered bushes and trees in the background. The sky above is wide and mostly clear, painted with a blue hue and streaks of wispy clouds. The scene is bright and peaceful, suggesting a pleasant day in a natural, open landscape.\n"
     ]
    }
   ],
   "source": [
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Responses API Streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_stream = client.responses.create(\n",
    "    model=model,\n",
    "    input=\"write a long essage on a topic of your own!\",\n",
    "    stream=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here’s a long message on the topic of **“The Importance of Cultivating Empathy in Today’s World.”**\n",
      "\n",
      "---\n",
      "\n",
      "In an increasingly interconnected but often divided world, cultivating empathy has never been more essential. Empathy—the ability to understand and share the feelings of another—is a fundamental human trait that fosters compassion, strengthens relationships, and promotes social harmony. As societies rapidly evolve with technological advancements, cultural exchanges, and global challenges, the need for empathy becomes a critical pillar for building a more just and inclusive world.\n",
      "\n",
      "One of the most powerful aspects of empathy is its ability to bridge differences. In many places, people are divided by race, religion, politics, socioeconomic status, or cultural norms. When empathy is practiced, individuals move beyond stereotypes and prejudices, seeking to understand the experiences and emotions of others. This understanding dismantles barriers, reduces conflict, and encourages cooperation even among people with divergent viewpoints. Empathy allows us to recognize our shared humanity, reminding us that underneath various external differences, everyone experiences joy, pain, hope, and fear.\n",
      "\n",
      "Moreover, empathy enhances personal relationships—from families and friendships to workplaces and communities. When people listen deeply and respond with genuine care, trust is built. In families, empathetic communication nurtures emotional security and resilience among members, especially children. In professional environments, empathy can improve teamwork, creativity, and productivity by promoting mutual respect and reducing misunderstandings. Leaders who lead with empathy inspire loyalty and motivate others, creating organizational cultures that prioritize well-being alongside achievement.\n",
      "\n",
      "On a societal level, empathy is a catalyst for social justice. Many of the world’s most pressing issues—such as poverty, inequality, discrimination, and environmental degradation—persist because people remain indifferent or disconnected from those suffering. Empathy invites individuals to step out of their comfort zones and advocate for change. By emotionally connecting with others’ struggles, people become more willing to support policies and initiatives that promote fairness and equity. This drive for empathy-informed activism is evident in movements championing human rights, refugee assistance, and climate action, where compassion translates into collective responsibility.\n",
      "\n",
      "Technological advancements present both opportunities and challenges for empathy. On one hand, digital platforms can facilitate meaningful exchanges between people from diverse backgrounds, fostering global empathy. On the other hand, the rise of social media can sometimes amplify echo chambers, misinformation, and dehumanization. In this context, it is vital to consciously practice empathy online—actively listening, questioning biases, and engaging respectfully. Digital empathy is the modern extension of this timeless virtue and requires mindful cultivation.\n",
      "\n",
      "Teaching empathy from an early age is another crucial step in building empathetic societies. Educational systems that integrate social-emotional learning help children develop self-awareness, emotional regulation, and perspective-taking skills. These competencies not only improve academic outcomes but also prepare future generations to be compassionate citizens and empathetic leaders. Parents, educators, and mentors play a vital role in modeling empathy, as children often learn best through observation and experience.\n",
      "\n",
      "In conclusion, empathy is more than just an emotional response; it is a skill and a choice that shapes how we relate to one another and to the world around us. Cultivating empathy brings about positive change at both personal and societal levels, fostering understanding, kindness, and justice. In a world that sometimes feels divided and uncertain, empathy remains a powerful force that can unite us, heal wounds, and inspire hope. By embracing empathy in our daily lives, we contribute to a future where compassion prevails, and humanity thrives.\n",
      "\n",
      "---\n",
      "\n",
      "If you’d like, I can also write on a different topic or in a different style! Just let me know."
     ]
    }
   ],
   "source": [
    "for event in response_stream:\n",
    "    if event.type == 'response.output_text.delta':\n",
    "        print(event.delta, end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"response\": {\n",
      "    \"id\": \"resp_68c25833df14819698e2f8a090afac81061c92d8112a0e6d\",\n",
      "    \"created_at\": 1757567027.0,\n",
      "    \"error\": null,\n",
      "    \"incomplete_details\": null,\n",
      "    \"instructions\": null,\n",
      "    \"metadata\": {},\n",
      "    \"model\": \"gpt-4.1-mini-2025-04-14\",\n",
      "    \"object\": \"response\",\n",
      "    \"output\": [\n",
      "      {\n",
      "        \"id\": \"msg_68c25834c86881968f8013492b8b194c061c92d8112a0e6d\",\n",
      "        \"content\": [\n",
      "          {\n",
      "            \"annotations\": [],\n",
      "            \"text\": \"Hello! How can I assist you today?\",\n",
      "            \"type\": \"output_text\",\n",
      "            \"logprobs\": []\n",
      "          }\n",
      "        ],\n",
      "        \"role\": \"assistant\",\n",
      "        \"status\": \"completed\",\n",
      "        \"type\": \"message\"\n",
      "      }\n",
      "    ],\n",
      "    \"parallel_tool_calls\": true,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tool_choice\": \"auto\",\n",
      "    \"tools\": [],\n",
      "    \"top_p\": 1.0,\n",
      "    \"background\": false,\n",
      "    \"max_output_tokens\": null,\n",
      "    \"max_tool_calls\": null,\n",
      "    \"previous_response_id\": null,\n",
      "    \"prompt_cache_key\": null,\n",
      "    \"reasoning\": {\n",
      "      \"effort\": null,\n",
      "      \"summary\": null\n",
      "    },\n",
      "    \"safety_identifier\": null,\n",
      "    \"service_tier\": \"default\",\n",
      "    \"status\": \"completed\",\n",
      "    \"text\": {\n",
      "      \"format\": {\n",
      "        \"type\": \"text\"\n",
      "      },\n",
      "      \"verbosity\": \"medium\"\n",
      "    },\n",
      "    \"top_logprobs\": 0,\n",
      "    \"truncation\": \"disabled\",\n",
      "    \"usage\": {\n",
      "      \"input_tokens\": 11,\n",
      "      \"input_tokens_details\": {\n",
      "        \"cached_tokens\": 0\n",
      "      },\n",
      "      \"output_tokens\": 10,\n",
      "      \"output_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0\n",
      "      },\n",
      "      \"total_tokens\": 21\n",
      "    },\n",
      "    \"user\": null,\n",
      "    \"store\": true\n",
      "  },\n",
      "  \"sequence_number\": 16,\n",
      "  \"type\": \"response.completed\"\n",
      "}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ResponseCompletedEvent' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(chunk.to_json())\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m.status)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MutazAbuGhazaleh\\miniconda3\\envs\\openai_env\\Lib\\site-packages\\pydantic\\main.py:991\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'ResponseCompletedEvent' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "print(chunk.to_json())\n",
    "print(chunk.item.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moderation models\n",
    "\n",
    "\n",
    "Classifies if text and/or image inputs are potentially harmful. \n",
    "\n",
    "https://platform.openai.com/docs/guides/moderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['omni-moderation-latest', 'omni-moderation-2024-09-26']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m.id for m in client.models.list().data if m.id.__contains__(\"mode\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"modr-CEPK9zRiUmu3n6D6My87Ab4M7PxRM\",\n",
      "  \"model\": \"text-moderation-007\",\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"categories\": {\n",
      "        \"harassment\": true,\n",
      "        \"harassment/threatening\": true,\n",
      "        \"hate\": false,\n",
      "        \"hate/threatening\": false,\n",
      "        \"self-harm\": false,\n",
      "        \"self-harm/instructions\": false,\n",
      "        \"self-harm/intent\": false,\n",
      "        \"sexual\": false,\n",
      "        \"sexual/minors\": false,\n",
      "        \"violence\": true,\n",
      "        \"violence/graphic\": false,\n",
      "        \"self-harm\": false,\n",
      "        \"sexual/minors\": false,\n",
      "        \"hate/threatening\": false,\n",
      "        \"violence/graphic\": false,\n",
      "        \"self-harm/intent\": false,\n",
      "        \"self-harm/instructions\": false,\n",
      "        \"harassment/threatening\": true\n",
      "      },\n",
      "      \"category_scores\": {\n",
      "        \"harassment\": 0.5215635299682617,\n",
      "        \"harassment/threatening\": 0.5694745779037476,\n",
      "        \"hate\": 0.22706663608551025,\n",
      "        \"hate/threatening\": 0.023547329008579254,\n",
      "        \"self-harm\": 2.227119921371923e-6,\n",
      "        \"self-harm/instructions\": 1.1198755256458526e-9,\n",
      "        \"self-harm/intent\": 1.646940972932498e-6,\n",
      "        \"sexual\": 0.000011726012417057063,\n",
      "        \"sexual/minors\": 7.107352217872176e-8,\n",
      "        \"violence\": 0.9971134662628174,\n",
      "        \"violence/graphic\": 0.00003391829886822961,\n",
      "        \"self-harm\": 2.227119921371923e-6,\n",
      "        \"sexual/minors\": 7.107352217872176e-8,\n",
      "        \"hate/threatening\": 0.023547329008579254,\n",
      "        \"violence/graphic\": 0.00003391829886822961,\n",
      "        \"self-harm/intent\": 1.646940972932498e-6,\n",
      "        \"self-harm/instructions\": 1.1198755256458526e-9,\n",
      "        \"harassment/threatening\": 0.5694745779037476\n",
      "      },\n",
      "      \"flagged\": true\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "moderation = client.moderations.create(input=\"I want to kill them.\")\n",
    "print(moderation.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load image from base64 file\n",
    "with open(\"image_base64_2.txt\", \"r\") as f:\n",
    "    image_base64 = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_moderation = client.moderations.create(\n",
    "    model=\"omni-moderation-latest\",\n",
    "    input=[\n",
    "        # {\"type\": \"text\", \"text\": \"...text to classify goes here...\"},\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{image_base64}\"\n",
    "            }\n",
    "        },\n",
    "    ],\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"modr-5219\",\n",
      "  \"model\": \"omni-moderation-latest\",\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"categories\": {\n",
      "        \"harassment\": false,\n",
      "        \"harassment/threatening\": false,\n",
      "        \"hate\": false,\n",
      "        \"hate/threatening\": false,\n",
      "        \"illicit\": false,\n",
      "        \"illicit/violent\": false,\n",
      "        \"self-harm\": false,\n",
      "        \"self-harm/instructions\": false,\n",
      "        \"self-harm/intent\": false,\n",
      "        \"sexual\": true,\n",
      "        \"sexual/minors\": false,\n",
      "        \"violence\": false,\n",
      "        \"violence/graphic\": false,\n",
      "        \"harassment/threatening\": false,\n",
      "        \"hate/threatening\": false,\n",
      "        \"illicit/violent\": false,\n",
      "        \"self-harm/intent\": false,\n",
      "        \"self-harm/instructions\": false,\n",
      "        \"self-harm\": false,\n",
      "        \"sexual/minors\": false,\n",
      "        \"violence/graphic\": false\n",
      "      },\n",
      "      \"category_applied_input_types\": {\n",
      "        \"harassment\": [],\n",
      "        \"harassment/threatening\": [],\n",
      "        \"hate\": [],\n",
      "        \"hate/threatening\": [],\n",
      "        \"illicit\": [],\n",
      "        \"illicit/violent\": [],\n",
      "        \"self-harm\": [\n",
      "          \"image\"\n",
      "        ],\n",
      "        \"self-harm/instructions\": [\n",
      "          \"image\"\n",
      "        ],\n",
      "        \"self-harm/intent\": [\n",
      "          \"image\"\n",
      "        ],\n",
      "        \"sexual\": [\n",
      "          \"image\"\n",
      "        ],\n",
      "        \"sexual/minors\": [],\n",
      "        \"violence\": [\n",
      "          \"image\"\n",
      "        ],\n",
      "        \"violence/graphic\": [\n",
      "          \"image\"\n",
      "        ],\n",
      "        \"harassment/threatening\": [],\n",
      "        \"hate/threatening\": [],\n",
      "        \"illicit/violent\": [],\n",
      "        \"self-harm/intent\": [\n",
      "          \"image\"\n",
      "        ],\n",
      "        \"self-harm/instructions\": [\n",
      "          \"image\"\n",
      "        ],\n",
      "        \"self-harm\": [\n",
      "          \"image\"\n",
      "        ],\n",
      "        \"sexual/minors\": [],\n",
      "        \"violence/graphic\": [\n",
      "          \"image\"\n",
      "        ]\n",
      "      },\n",
      "      \"category_scores\": {\n",
      "        \"harassment\": 0.0,\n",
      "        \"harassment/threatening\": 0.0,\n",
      "        \"hate\": 0.0,\n",
      "        \"hate/threatening\": 0.0,\n",
      "        \"illicit\": 0.0,\n",
      "        \"illicit/violent\": 0.0,\n",
      "        \"self-harm\": 0.004667502552721809,\n",
      "        \"self-harm/instructions\": 0.00022628109184047356,\n",
      "        \"self-harm/intent\": 0.0002936668209450264,\n",
      "        \"sexual\": 0.9997110418982227,\n",
      "        \"sexual/minors\": 0.0,\n",
      "        \"violence\": 0.08889509449379349,\n",
      "        \"violence/graphic\": 0.0015680118993480474,\n",
      "        \"harassment/threatening\": 0.0,\n",
      "        \"hate/threatening\": 0.0,\n",
      "        \"illicit/violent\": 0.0,\n",
      "        \"self-harm/intent\": 0.0002936668209450264,\n",
      "        \"self-harm/instructions\": 0.00022628109184047356,\n",
      "        \"self-harm\": 0.004667502552721809,\n",
      "        \"sexual/minors\": 0.0,\n",
      "        \"violence/graphic\": 0.0015680118993480474\n",
      "      },\n",
      "      \"flagged\": true\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "moderation_response = image_moderation.to_json()\n",
    "print(image_moderation.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "\n",
    "moderation_data = json.loads(moderation_response, object_hook=lambda d: SimpleNamespace(**d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[namespace(categories=namespace(harassment=False,\n",
       "                                harassment/threatening=False,\n",
       "                                hate=False,\n",
       "                                hate/threatening=False,\n",
       "                                illicit=False,\n",
       "                                illicit/violent=False,\n",
       "                                self-harm=False,\n",
       "                                self-harm/instructions=False,\n",
       "                                self-harm/intent=False,\n",
       "                                sexual=True,\n",
       "                                sexual/minors=False,\n",
       "                                violence=False,\n",
       "                                violence/graphic=False),\n",
       "           category_applied_input_types=namespace(harassment=[],\n",
       "                                                  harassment/threatening=[],\n",
       "                                                  hate=[],\n",
       "                                                  hate/threatening=[],\n",
       "                                                  illicit=[],\n",
       "                                                  illicit/violent=[],\n",
       "                                                  self-harm=['image'],\n",
       "                                                  self-harm/instructions=['image'],\n",
       "                                                  self-harm/intent=['image'],\n",
       "                                                  sexual=['image'],\n",
       "                                                  sexual/minors=[],\n",
       "                                                  violence=['image'],\n",
       "                                                  violence/graphic=['image']),\n",
       "           category_scores=namespace(harassment=0.0,\n",
       "                                     harassment/threatening=0.0,\n",
       "                                     hate=0.0,\n",
       "                                     hate/threatening=0.0,\n",
       "                                     illicit=0.0,\n",
       "                                     illicit/violent=0.0,\n",
       "                                     self-harm=0.004667502552721809,\n",
       "                                     self-harm/instructions=0.00022628109184047356,\n",
       "                                     self-harm/intent=0.0002936668209450264,\n",
       "                                     sexual=0.9997110418982227,\n",
       "                                     sexual/minors=0.0,\n",
       "                                     violence=0.08889509449379349,\n",
       "                                     violence/graphic=0.0015680118993480474),\n",
       "           flagged=True)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moderation_data.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
